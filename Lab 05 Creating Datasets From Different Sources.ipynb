{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Datasets from different sources\n",
    "\n",
    "In this reading notebook, we will explore a few of the ways in which we can load data into a `tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `from_tensor_slices` and `from_tensors` methods\n",
    "\n",
    "We will start by looking at the `from_tensor_slices` and the `from_tensors` methods.\n",
    "\n",
    "Both static methods are used to create datasets from Tensors or Tensor-like objects, such as numpy arrays or python lists. We can also pass in tuples and dicts of arrays or lists. The main distinction between the `from_tensor_slices` function and the `from_tensors` function is that the `from_tensor_slices` method will interpret the first dimension of the input data as the number of elements in the dataset, whereas the `from_tensors` method always results in a Dataset with a single element, containing the Tensor or tuple of Tensors passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random tensor with shape (3, 2)\n",
    "\n",
    "example_tensor = tf.random.uniform([3,2])\n",
    "print(example_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two Datasets, using each static method\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(example_tensor)\n",
    "dataset2 = tf.data.Dataset.from_tensors(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec for each\n",
    "\n",
    "print(dataset1.element_spec)\n",
    "print(dataset2.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, creating the Dataset using the `from_tensor_slices` method slices the given array or Tensor along the first dimension to produce a set of elements for the Dataset.\n",
    "\n",
    "This means that although we could pass any Tensor - or tuple of Tensors - to the `from_tensors` method, the same cannot be said of the `from_tensor_slices` method, which has the additional requirement that each Tensor in the list has the same size in the zeroth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three Tensors with different shapes\n",
    "\n",
    "tensor1 = tf.random.uniform([10,2,2])\n",
    "tensor2 = tf.random.uniform([10,1])\n",
    "tensor3 = tf.random.uniform([9,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot create a Dataset using the `from_tensor_slices` method from a list of `tensor1` and `tensor3` since they do not have the same size in the first dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create a Dataset from tensor1 and tensor3 using from_tensor_slices - this will raise an error\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tensor1, tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can of course create a Dataset from this tuple using the `from_tensors` method, which interprets the tuple as a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Dataset from tensor1 and tensor3 using from_tensors\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors((tensor1, tensor3))\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although `tensor1` and `tensor2` do not have the same shape, or even same rank (number of dimensions), we can still use the `from_tensor_slices` method to form a dataset from a list of these tensors, since they have the same size in the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from tensor1 and tensor2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tensor1, tensor2))\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the first dimension was interpreted as the number of elements in the Dataset, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets from numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `from_tensor_slices` and `from_tensors` methods to create Datasets from numpy arrays. In fact, behind the scenes, the numpy array is converted to a set of `tf.constant` operations to populate the Tensor in the TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "numpy_array = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "print(numpy_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two Datasets, using each static method\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(numpy_array)\n",
    "dataset2 = tf.data.Dataset.from_tensors(numpy_array)\n",
    "\n",
    "print(dataset1.element_spec)\n",
    "print(dataset2.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, `from_tensors` interprets the entire array as a single element, whereas `from_tensor_slices` slices the array along the first dimension to form the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets from pandas DataFrames\n",
    "A pandas DataFrame can be easily converted to a Dataset using the `from_tensor_slices` method. \n",
    "#### The Balloons dataset\n",
    "A pandas DataFrame can be loaded from a CSV file. We will use the [Balloons dataset](https://archive.ics.uci.edu/ml/datasets/Balloons) to demonstrate. This dataset is stored in a CSV file, and contains a list of attributes describing instances of a balloon inflation experiment, such as the colour and size of the balloon, the age of the person who performed the attempted inflation, and the way in which they did it. Finally, there is the target column \"Inflated\", which is either `T` for True, or `F` for False, indicating whether or not the person managed to inflate the balloon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pandas_dataframe = pd.read_csv('data/balloon_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "\n",
    "pandas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the DataFrame to a Dataset, we first convert the DataFrame to a dictionary. By doing this, we preserve the column names as the dictionary labels.\n",
    "\n",
    "**Note**: A Dataset can be formed from either a tuple or a dict of Tensors. We saw above a number of Datasets being formed from a tuple. The only distinction for a Dataset formed from a dict is that the Dataset items will be dicts accessed by key, rather than tuples accessed by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a dict\n",
    "\n",
    "dataframe_dict = dict(pandas_dataframe)\n",
    "print(dataframe_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the `from_tensor_slices` method on this `dict` and print the resulting Dataset `element_spec`, as well as an example element. Note that since we formed the Dataset from a `dict`, we see the column (dictionary) names in the `element_spec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dataset\n",
    "\n",
    "pandas_dataset = tf.data.Dataset.from_tensor_slices(dataframe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Dataset element_spec\n",
    "\n",
    "pandas_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate the Dataset\n",
    "\n",
    "next(iter(pandas_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets directly from CSV Files\n",
    "\n",
    "The TensorFlow experimental library contains a variety of functions and classes contributed by the community that may not be ready for release into the main TensorFlow library in their immediate form, but which may be included in TensorFlow in the future. One such useful experimental function is the `tf.data.experimental.make_csv_dataset` function. This allows us to read CSV data from the disk directly into a Dataset object.\n",
    "\n",
    "We will run the function on the example CSV file from disk, and specify the batch size and the name of the target column, which is used to structure the Dataset into an `(input, target)` tuple.\n",
    "\n",
    "**Note:** Because of the ephemeral nature of the `experimental` package, you may well get warnings printed in the console when using a function or class contained in the package for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the Dataset from the CSV file\n",
    "\n",
    "csv_dataset = tf.data.experimental.make_csv_dataset('data/balloon_dataset.csv',\n",
    "                                                    batch_size=1,\n",
    "                                                    label_name='Inflated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that we've loaded our Dataset correctly, let's print the `element_spec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Dataset element_spec\n",
    "\n",
    "csv_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate the Dataset\n",
    "\n",
    "next(iter(csv_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above Dataset, the target column `Inflated` does not have a key, since it is uniquely accessible as the second element of the tuple, whereas the attributes which reside as a dictionary of Tensors in the first element retain their labels so we can distinguish them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and resources\n",
    "\n",
    "* https://www.tensorflow.org/guide/data\n",
    "* https://www.tensorflow.org/tutorials/load_data/csv\n",
    "* https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
    "* https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
