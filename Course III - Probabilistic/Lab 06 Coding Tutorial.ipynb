{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "print('TF version:', tf.__version__)\n",
    "print('TFP version:', tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic layers and Bayesian neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding tutorials\n",
    "#### [1. The DistributionLambda layer](#coding_tutorial_1)\n",
    "#### [2. Probabilistic layers](#coding_tutorial_2)\n",
    "#### [3. The DenseVariational layer](#coding_tutorial_3)\n",
    "#### [4. Reparameterization layers](#coding_tutorial_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## The `DistributionLambda` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a probabilistic model using the `DistributionLambda` layer\n",
    "\n",
    "Create a model whose first layer represents:\n",
    "\n",
    "$$\n",
    "y = \\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sigmoid model, first deterministic, then probabilistic\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(1,), units=1, activation='sigmoid',\n",
    "          kernel_initializer=tf.constant_initializer(1),\n",
    "          bias_initializer=tf.constant_initializer(0)),\n",
    "])\n",
    "\n",
    "# Plot the function\n",
    "x_plot = np.linspace(-5, 5, 100)\n",
    "plt.scatter(x_plot, model.predict(x_plot), alpha=0.4)\n",
    "plt.plot(x_plot, 1/(1 + np.exp(-x_plot)), color='r', alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a constant input for this model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the feedforward object...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ... and its behaviour under repeated calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the forward model to create probabilistic training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to create 500 training points\n",
    "\n",
    "x_train = np.linspace(-5, 5, 500)[:, np.newaxis]\n",
    "y_train = model.predict(x_train)\n",
    "\n",
    "# Plot the data and the mean of the distribution\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(x_train, y_train, alpha=0.04, color='blue', label='samples')\n",
    "ax.plot(x_train, model(x_train).mean().numpy().flatten(), \n",
    "        color='red', alpha=0.8, label='mean')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new probabilistic model with the wrong weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new version of the model, with the wrong weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the new model with the negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define negative loglikelihood, which we will use for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile untrained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, record weights after each epoch\n",
    "\n",
    "epochs = [0]\n",
    "training_weights = [model_untrained.weights[0].numpy()[0, 0]]\n",
    "training_bias = [model_untrained.weights[1].numpy()[0]]\n",
    "for epoch in range(100):\n",
    "    model_untrained.fit(x=x_train, y=y_train, epochs=1, verbose=False)\n",
    "    epochs.append(epoch)\n",
    "    training_weights.append(model_untrained.weights[0].numpy()[0, 0])\n",
    "    training_bias.append(model_untrained.weights[1].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model weights as they train, converging to the correct values\n",
    "\n",
    "plt.plot(epochs, training_weights, label='weight')\n",
    "plt.plot(epochs, training_bias, label='bias')\n",
    "plt.axhline(y=1, label='true_weight', color='k', linestyle=':')\n",
    "plt.axhline(y=0, label='true_bias', color='k', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Probabilistic layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data\n",
    "\n",
    "The data you'll be working with is artifically created from the following equation:\n",
    "$$ y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
    "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot 100 points of training data\n",
    "\n",
    "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
    "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.4)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deterministic linear regression with MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train deterministic linear model using mean squared error loss\n",
    "\n",
    "# Create linear regression via Sequential model\n",
    "model = Sequential([\n",
    "    Dense(units=1, input_shape=(1,))\n",
    "])\n",
    "model.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=200, verbose=False)\n",
    "\n",
    "# Plot the data and model\n",
    "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "plt.plot(x_train, model.predict(x_train), color='red', alpha=0.8, label='model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the model predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic linear regression with both user-defined and learned variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression with normal distribution as final layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using the negative loglikelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the distribution created as a feedforward value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and a sample from the model\n",
    "\n",
    "y_model = model(x_train)\n",
    "y_sample = y_model.sample()\n",
    "y_hat = y_model.mean()\n",
    "y_sd = y_model.stddev()\n",
    "y_hat_m2sd = y_hat - 2 * y_sd\n",
    "y_hat_p2sd = y_hat + 2 * y_sd\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax1.scatter(x_train, y_sample, alpha=0.4, color='red', label='model sample')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
    "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic linear regression with nonlinear learned mean & variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the data to being nonlinear:\n",
    "$$ y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i$$\n",
    "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot 10000 data points\n",
    "\n",
    "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
    "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression: normal distribution with fixed variance\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(1,), units=8, activation='sigmoid'),\n",
    "    Dense(tfpl.IndependentNormal.params_size(event_shape=1)),\n",
    "    tfpl.IndependentNormal(event_shape=1)\n",
    "])\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model.fit(x_train, y_train, epochs=200, verbose=False)\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and a sample from the model\n",
    "\n",
    "y_model = model(x_train)\n",
    "y_sample = y_model.sample()\n",
    "y_hat = y_model.mean()\n",
    "y_sd = y_model.stddev()\n",
    "y_hat_m2sd = y_hat - 2 * y_sd\n",
    "y_hat_p2sd = y_hat + 2 * y_sd\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.2, label='data')\n",
    "ax1.scatter(x_train, y_sample, alpha=0.2, color='red', label='model sample')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.2, label='data')\n",
    "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
    "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## The `DenseVariational` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create linear data with Gaussian noise\n",
    "\n",
    "The data you'll be working with is the same as you used before:\n",
    "$$ y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
    "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed. We'll be running a Bayesian linear regression on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same data as before -- create and plot 100 data points\n",
    "\n",
    "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
    "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the prior and posterior distribution for model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior weight distribution -- all N(0, 1) -- and not trainable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variational posterior weight distribution -- multivariate Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: analytical posterior\n",
    "\n",
    "In this tutorial, we're using a variational posterior because, in most settings, it's not possible to derive an analytical one. However, in this simple setting, it is possible. Specifically, running a Bayesian linear regression on $x_i$ and $y_i$ with $i=1, \\ldots, n$ and a unit Gaussian prior on both $\\alpha$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "y_i = \\alpha + \\beta x_i + \\epsilon_i, \\quad \n",
    "\\epsilon_i \\sim N(0, \\sigma^2), \\quad \n",
    "\\alpha \\sim N(0, 1), \\quad \n",
    "\\beta \\sim N(0, 1)\n",
    "$$\n",
    "\n",
    "gives a multivariate Gaussian posterior on $\\alpha$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "N(\\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
    "$$\n",
    "where\n",
    "$$ \n",
    "\\mathbf{\\mu}\n",
    "= \n",
    "\\mathbf{\\Sigma} \n",
    "\\begin{pmatrix}\n",
    "\\hat{n} \\bar{y} \\\\\n",
    "\\hat{n} \\overline{xy}\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\mathbf{\\Sigma} = \n",
    "\\frac{1}{(\\hat{n} + 1)(\\hat{n} \\overline{x^2} + 1) - \\hat{n}^2 \\bar{x}^2}\n",
    "\\begin{pmatrix}\n",
    "\\hat{n} \\overline{x^2} + 1 & -\\hat{n} \\bar{x} \\\\\n",
    "-\\hat{n} \\bar{x} & \\hat{n} + 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "In the above, $\\hat{n} = \\frac{n}{\\sigma^2}$ and $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^n t_i$ for any $t$. In general, however, it's not possible to determine the analytical form for the posterior. For example, in models with a hidden layer with nonlinear activation function, the analytical posterior cannot be determined in general, and variational methods as below are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model with `DenseVariational` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression model with weight uncertainty: weights are\n",
    "# distributed according to posterior (and, indirectly, prior) distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model, just like a deterministic linear regression\n",
    "\n",
    "model.fit(x_train, y_train, epochs=500, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the parameters of the prior and posterior distribution\n",
    "\n",
    "dummy_input = np.array([[0]])\n",
    "model_prior = model.layers[0]._prior(dummy_input)\n",
    "model_posterior = model.layers[0]._posterior(dummy_input)\n",
    "print('prior mean:           ', model_prior.mean().numpy())\n",
    "print('prior variance:       ', model_prior.variance().numpy())\n",
    "print('posterior mean:       ', model_posterior.mean().numpy())\n",
    "print('posterior covariance: ', model_posterior.covariance().numpy()[0])\n",
    "print('                      ', model_posterior.covariance().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an ensemble of linear regressions, with weights sampled from\n",
    "# the posterior distribution\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "for _ in range(10):\n",
    "    y_model = model(x_train)\n",
    "    if _ == 0:\n",
    "        plt.plot(x_train, y_model, color='red', alpha=0.8, label='model')\n",
    "    else:\n",
    "        plt.plot(x_train, y_model, color='red', alpha=0.8)        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the effect of sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets, one with 1000 points, another with 100\n",
    "\n",
    "x_train_1000 = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
    "y_train_1000 = x_train_1000 + 0.3*np.random.randn(1000)[:, np.newaxis]\n",
    "\n",
    "x_train_100 = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
    "y_train_100 = x_train_100 + 0.3*np.random.randn(100)[:, np.newaxis]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "ax1.scatter(x_train_1000, y_train_1000, alpha=0.1)\n",
    "ax2.scatter(x_train_100, y_train_100, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on each dataset\n",
    "\n",
    "model_1000 = Sequential([tfpl.DenseVariational(input_shape=(1,), \n",
    "                                               units=1,\n",
    "                                               make_prior_fn=prior, \n",
    "                                               make_posterior_fn=posterior,\n",
    "                                               kl_weight=1/1000)])\n",
    "\n",
    "model_100 = Sequential([tfpl.DenseVariational(input_shape=(1,), \n",
    "                                              units=1,\n",
    "                                              make_prior_fn=prior, \n",
    "                                              make_posterior_fn=posterior,\n",
    "                                              kl_weight=1/100)])\n",
    "\n",
    "model_1000.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
    "model_100.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
    "\n",
    "model_1000.fit(x_train_1000, y_train_1000, epochs=50, verbose=False)\n",
    "model_100.fit(x_train_100, y_train_100, epochs=500, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an ensemble of linear regressions from each model\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "for _ in range(10):\n",
    "    y_model_1000 = model_1000(x_train_1000)\n",
    "    ax1.scatter(x_train_1000, y_train_1000, color='C0', alpha=0.02)\n",
    "    ax1.plot(x_train_1000, y_model_1000, color='red', alpha=0.8)\n",
    "    y_model_100 = model_100(x_train_100)\n",
    "    ax2.scatter(x_train_100, y_train_100, color='C0', alpha=0.05)\n",
    "    ax2.plot(x_train_100, y_model_100, color='red', alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it all together: nonlinear probabilistic regression with weight uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the data to being nonlinear:\n",
    "$$ y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i$$\n",
    "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot 1000 data points\n",
    "\n",
    "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
    "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression with one hidden layer, weight uncertainty\n",
    "\n",
    "model = Sequential([\n",
    "    tfpl.DenseVariational(units=8,\n",
    "                          input_shape=(1,),\n",
    "                          make_prior_fn=prior,\n",
    "                          make_posterior_fn=posterior,\n",
    "                          kl_weight=1/x_train.shape[0],\n",
    "                          activation='sigmoid'),\n",
    "    tfpl.DenseVariational(units=tfpl.IndependentNormal.params_size(1),\n",
    "                          make_prior_fn=prior,\n",
    "                          make_posterior_fn=posterior,\n",
    "                          kl_weight=1/x_train.shape[0]),\n",
    "    tfpl.IndependentNormal(1)\n",
    "])\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.005))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, verbose=False)\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an ensemble of trained probabilistic regressions\n",
    "\n",
    "plt.scatter(x_train, y_train, marker='.', alpha=0.2, label='data')\n",
    "for _ in range(5):\n",
    "    y_model = model(x_train)\n",
    "    y_hat = y_model.mean()\n",
    "    y_hat_m2sd = y_hat - 2 * y_model.stddev()\n",
    "    y_hat_p2sd = y_hat + 2 * y_model.stddev()\n",
    "    if _ == 0:\n",
    "        plt.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
    "        plt.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
    "        plt.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
    "    else:\n",
    "        plt.plot(x_train, y_hat, color='red', alpha=0.8)\n",
    "        plt.plot(x_train, y_hat_m2sd, color='green', alpha=0.8)\n",
    "        plt.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## Reparameterization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the HAR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be working with the [Human Activity Recognition (HAR) Using Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) dataset. It consists of the readings from an accelerometer (which measures acceleration) carried by a human doing different activities. The six activities are walking horizontally, walking upstairs, walking downstairs, sitting, standing and laying down. The accelerometer is inside a smartphone, and, every 0.02 seconds (50 times per second), it takes six readings: linear and gyroscopic acceleration in the x, y and z directions. See [this link](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) for details and download. If you use it in your own research, please cite the following paper:\n",
    "\n",
    "- Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \n",
    "\n",
    "The goal is to use the accelerometer data to predict the activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HAR dataset and create some data processing functions\n",
    "\n",
    "# Function to load the data from file\n",
    "def load_HAR_data():\n",
    "    data_dir = 'data/HAR/'\n",
    "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))[..., :6]\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy')) - 1\n",
    "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))[..., :6]\n",
    "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy')) - 1\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Dictionary containing the labels and the associated activities\n",
    "label_to_activity = {0: 'walking horizontally', 1: 'walking upstairs', 2: 'walking downstairs',\n",
    "                     3: 'sitting', 4: 'standing', 5: 'laying'}\n",
    "\n",
    "# Function to change integer labels to one-hot labels\n",
    "def integer_to_onehot(data_integer):\n",
    "    data_onehot = np.zeros(shape=(data_integer.shape[0], data_integer.max()+1))\n",
    "    for row in range(data_integer.shape[0]):\n",
    "        integer = int(data_integer[row])\n",
    "        data_onehot[row, integer] = 1\n",
    "    return data_onehot\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = load_HAR_data()\n",
    "y_train_oh = integer_to_onehot(y_train)\n",
    "y_test_oh = integer_to_onehot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some of the data by making plots\n",
    "\n",
    "def make_plots(num_examples_per_category):\n",
    "    for label in range(6):\n",
    "        x_label = x_train[y_train[:, 0] == label]\n",
    "        for i in range(num_examples_per_category):\n",
    "            fig, ax = plt.subplots(figsize=(10, 1))\n",
    "            ax.imshow(x_label[100*i].T, cmap='Greys', vmin=-1, vmax=1)\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title(label_to_activity[label])\n",
    "            plt.show()\n",
    "        \n",
    "make_plots(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D deterministic convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard deterministic model with:\n",
    "# - Conv1D\n",
    "# - MaxPooling\n",
    "# - Flatten\n",
    "# - Dense with Softmax\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(input_shape=(128, 6), filters=8, kernel_size=16, activation='relu'),\n",
    "    MaxPooling1D(pool_size=16),\n",
    "    Flatten(),\n",
    "    Dense(units=6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic 1D convolutional neural network, with both weight and output uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probablistic model with the following layers:\n",
    "#  - Conv1D\n",
    "#  - MaxPooling\n",
    "#  - Flatten\n",
    "#  - Dense\n",
    "#  - OneHotCategorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace analytical Kullback-Leibler divergence with approximated one\n",
    "\n",
    "def kl_approx(q, p, q_tensor):\n",
    "    return tf.reduce_mean(q.log_prob(q_tensor) - p.log_prob(q_tensor))\n",
    "\n",
    "divergence_fn = lambda q, p, q_tensor : kl_approx(q, p, q_tensor) / x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using the negative loglikelihood\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "model.compile(loss=nll,\n",
    "              optimizer=RMSprop(learning_rate=0.005),\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(x_train, y_train_oh, epochs=20, verbose=False)\n",
    "model.evaluate(x_train, y_train_oh)\n",
    "model.evaluate(x_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to analyse model predictions versus true labels\n",
    "\n",
    "def analyse_model_predictions(image_num):\n",
    "\n",
    "    # Show the accelerometer data\n",
    "    print('------------------------------')\n",
    "    print('Accelerometer data:')\n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    ax.imshow(x_test[image_num].T, cmap='Greys', vmin=-1, vmax=1)\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the true activity\n",
    "    print('------------------------------')\n",
    "    print('True activity:', label_to_activity[y_test[image_num, 0]])\n",
    "    print('')\n",
    "\n",
    "    # Print the probabilities the model assigns\n",
    "    print('------------------------------')\n",
    "    print('Model estimated probabilities:')\n",
    "    # Create ensemble of predicted probabilities\n",
    "    predicted_probabilities = np.empty(shape=(200, 6))\n",
    "    for i in range(200):\n",
    "        predicted_probabilities[i] = model(x_test[image_num][np.newaxis, ...]).mean().numpy()[0]\n",
    "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(6)])\n",
    "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(6)])\n",
    "    # Make the plots\n",
    "    fig, ax = plt.subplots(figsize=(9, 3))\n",
    "    bar = ax.bar(np.arange(6), pct_97p5, color='red')\n",
    "    bar[y_test[image_num, 0]].set_color('green')\n",
    "    bar = ax.bar(np.arange(6), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n",
    "    ax.set_xticklabels([''] + [activity for activity in label_to_activity.values()],\n",
    "                       rotation=45, horizontalalignment='right')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('Probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
